# Roadmap

## Phase 0: Foundations  **[DONE]**
- Repo scaffold, docs, minimal FastAPI (/health)  ‚úÖ Implemented (FastAPI app, /health, basic logging)

## Phase 1: Core Ingestion & Metadata  **[PARTIAL]**
- Implement hashing + EXIF extraction  ‚úÖ (hashing + EXIF in pipeline)
- SQLite schema & migrations  ‚úÖ (Alembic set up; multiple revisions)
 - CLI ingest command  ‚úÖ (Typer CLI `ingest-scan` added; further expansion pending)
- Dedup logic  ‚úÖ (sha256 + perceptual hash + near-duplicate clustering endpoint)

## Phase 2: Thumbnails & Embeddings (Search v1)  **[PARTIAL]**
- Thumbnail generator  ‚úÖ (thumb task implemented)
- Embedding service (image/text)  ‚úÖ Real model support (CLIP via open_clip / sentence-transformers) with stub fallback
- Embedding version + device tracking  ‚úÖ (model, dim, device, optional version stored)
- Re-embed scheduling on model change  ‚úÖ (startup detection & task enqueue)
- Vector search endpoint  ‚úÖ (text or image-based)
- Persistent vector index (FAISS Flat IP)  ‚úÖ (memory or faiss backend selectable)
- Approximate / optimized ANN (HNSW / IVF)  ‚ùå Not started

## Phase 3: Captions & Hybrid Search  **[COMPLETED - PRODUCTION READY]**
- BLIP2 caption generation  ‚úÖ **PRODUCTION READY** ‚Äî Full backend integration with 13.96 GB local BLIP2 model operational
- External caption model architecture  ‚úÖ **COMPLETED** ‚Äî Dual environment architecture with subprocess communication via JSON
- Backend integration  ‚úÖ **COMPLETED** ‚Äî FastAPI server running with BLIP2SubprocessProvider, health endpoints validated
- Local model storage  ‚úÖ **COMPLETED** ‚Äî Models moved from cache to project directory (20.96 GB total), dependency-free
- Command-line interface  ‚úÖ **COMPLETED** ‚Äî Backend-compatible inference script with argument parsing
- Model fallback system  ‚úÖ **AVAILABLE** ‚Äî Infrastructure supports multiple models (BLIP2 primary, Qwen2.5-VL secondary)
- Text embedding index  ‚è≥ Available via existing embedding service (ready for caption integration)
- Hybrid ranking weights config  üìã **NEXT** ‚Äî Awaiting end-to-end testing completion

## Phase 4: Faces & Person Albums  **[COMPLETED]**
- Face detection  ‚úÖ MTCNN provider implemented + configurable (stub/mtcnn/auto)
- Face embeddings  ‚úÖ **REAL MODELS** ‚Äî Multiple providers: Facenet, InsightFace, LVFace (subprocess mode for external models)
- Clustering (incremental + full recluster w/ progress + cancel)  ‚úÖ Implemented
- Labeling & merge/split workflow  ‚úÖ **COMPLETED** ‚Äî Full person management API with rename, merge, split operations
- Person-based search / filtering  ‚úÖ **COMPLETED** ‚Äî Multiple search endpoints: by ID, by name, vector search with person filter

## Phase 5: Events & Themes  **[NOT STARTED]**
- Event segmentation  ‚ùå
- Theme clustering & labeling  ‚ùå
- Smart album DSL  ‚ùå

## Phase 6: Voice & Advanced UX  **[NOT STARTED]**
- Whisper integration for voice annotations  ‚ùå
- Album summarization  ‚ùå
- Query suggestions & relevance feedback  ‚ùå

## Phase 7: Optimization & Hardening  **[NOT STARTED]**
- GPU scheduling & batching improvements  ‚ùå
- Postgres migration option  ‚ùå (design consideration only)
- Backup & restore tooling  ‚ùå

## Phase 8: Extended Media & Analytics  **[NOT STARTED]**
- Short video keyframe support  ‚ùå
- Usage analytics dashboards  ‚ùå

---

### Milestone v0.1.0 (mac smoke) ‚Äî DONE
‚Ä¢ Docker Desktop on macOS verified healthy (hello-world) and compose CLI working (`cli ping`).
‚Ä¢ Fixed compose CLI entrypoint/command; backend `requirements.txt` includes core deps by default.
‚Ä¢ Docs: Added Docker/VPN troubleshooting note (Clash/proxies) in README and deployment guide.
‚Ä¢ Dev ergonomics: split requirements (core vs ML), INCLUDE_ML toggle, mac smoke defaults.

---

## üéØ Next Steps (Current Focus)

> **Updated: 2025-08-15** ‚Äî After completing real caption generation system (BLIP2 operational)

### **Current Caption System Status** (2025-08-16)

**‚úÖ COMPLETED: Production-Ready Caption System**
- **Backend Integration**: FastAPI server operational with BLIP2SubprocessProvider
- **Local Model Storage**: 20.96 GB models moved from Hugging Face cache to `vlmCaptionModels/models/`
- **Dual Environment Architecture**: Isolated Python environments (vlmPhotoHouse/.venv + vlmCaptionModels/.venv)
- **Backend-Compatible Interface**: Created `inference_backend.py` with command-line argument parsing for JSON communication
- **Server Configuration**: Environment variables properly configured (CAPTION_PROVIDER=blip2, CAPTION_EXTERNAL_DIR, etc.)
- **Health Monitoring**: Both `/health` and `/health/caption` endpoints operational and validated

**‚úÖ WORKING: BLIP2 Caption Generation System**
- **Model**: Salesforce/blip2-opt-2.7b (13.96 GB) fully operational in local storage
- **Provider**: BLIP2SubprocessProvider confirmed working via health endpoints
- **Integration**: Backend can communicate with external caption environment via subprocess calls
- **Status**: Ready for production use - server running and responding to health checks
- **Location**: `vlmCaptionModels/inference_backend.py` (backend-compatible interface)

**‚úÖ RESOLVED: Model Management & Storage**
- **Cache Cleanup**: Successfully cleaned Hugging Face cache, freed 22+ GB of disk space
- **Local Storage**: Both BLIP2 (13.96 GB) and Qwen2.5-VL (7.00 GB) models stored locally
- **Path Issues**: Resolved PowerShell path execution errors and directory persistence
- **Subprocess Integration**: Fixed backend-compatible interface mismatch between JSON and command-line protocols

**üìÅ External Model Structure**
```
vlmCaptionModels/
‚îú‚îÄ‚îÄ .venv/                     # Isolated Python environment
‚îú‚îÄ‚îÄ inference.py               # Smart inference (tries Qwen2.5-VL ‚Üí BLIP2)
‚îú‚îÄ‚îÄ inference_blip2.py         # BLIP2-only inference
‚îú‚îÄ‚îÄ inference_smart.py         # Copy of smart inference
‚îú‚îÄ‚îÄ models/                    # Cached model files
‚îî‚îÄ‚îÄ requirements.txt           # Caption-specific dependencies
```

**üîó Integration Ready**
- Caption system tested and operational via JSON interface
- Ready to integrate with main photo processing pipeline
- Health checks and error handling implemented
- Multi-model architecture allows easy addition of new caption providers

### **Current Milestone: Complete Caption Pipeline Integration**

**Status**: ‚úÖ **CAPTION MODELS COMPLETED** ‚Äî Real caption generation system operational with smart fallback

**Next Priority: Pipeline Integration & Search Enhancement**

**Priority Order:**
1. **Person Management Workflow** ‚úÖ
   - **COMPLETED**: Face ‚Üí Person assignment logic
   - **COMPLETED**: API endpoints: rename person, merge persons, split persons  
   - **COMPLETED**: Person entity management with full CRUD operations

2. **Person-Based Search** ‚úÖ  
   - **COMPLETED**: Dedicated endpoints: "find photos of John" (`/search/person/{id}`, `/search/person/name/{name}`)
   - **COMPLETED**: Connect face embeddings ‚Üí person entities ‚Üí search results
   - **COMPLETED**: Filter search results by person with vector search integration (`/search/person/vector`)

3. **Real Caption Generation** ‚úÖ **MAJOR MILESTONE COMPLETED**
   - **COMPLETED**: Smart multi-model caption system (Qwen2.5-VL + BLIP2)
   - **COMPLETED**: External subprocess architecture with isolated Python environment  
   - **COMPLETED**: Intelligent model fallback (tries Qwen2.5-VL, falls back to BLIP2)
   - **COMPLETED**: JSON communication protocol and health monitoring
   - **STATUS**: BLIP2 fully operational, Qwen2.5-VL has model compatibility issues (shape mismatch)
   - **READY**: Caption generation available for integration with main pipeline

### **Next Development Priorities**

**Status**: ‚úÖ **CAPTION INTEGRATION COMPLETED** ‚Äî Full caption system operational and ready for end-to-end testing

4. **End-to-End Caption Testing** üìã **[NEXT IMMEDIATE PRIORITY]**
   - Test caption generation with real photos via `/ingest/scan` endpoint
   - Validate caption storage in database and task processing
   - Verify caption-based search functionality works end-to-end
   - Performance testing with multiple photos to ensure subprocess stability
   - **Ready**: All infrastructure complete, just needs real-world validation

5. **Caption Storage & Search Enhancement** üìã
   - Implement caption-based text search functionality
   - Integrate caption embeddings with image embeddings for hybrid search
   - Add caption filtering and ranking capabilities to search API
   - **Dependencies**: End-to-end testing completion

6. **Qwen2.5-VL Model Resolution** üìã **[OPTIONAL - BLIP2 SUFFICIENT]**
   - Debug Qwen2.5-VL model loading issues (shape mismatch between variants)
   - Test with different model configurations or versions
   - **Priority**: Low - BLIP2 provides full functionality for current needs
   - **Status**: Deferred until capacity allows

7. **User Interface & Experience** üìã
   - Web interface for photo browsing and search with caption display
   - Person labeling and album management UI
   - Photo organization and tagging interface with caption editing
   - Automatic event detection (time/location clustering)
   - Smart album generation based on people, events, themes
   - Album sharing and export capabilities

8. **End-to-End Integration Testing** üìã
   - Full pipeline test: photo ‚Üí faces ‚Üí embeddings ‚Üí clusters ‚Üí persons ‚Üí **captions** ‚Üí search
   - Validate complete workflow with real photos

**Rationale**: Person management is the missing link between working face detection and user-facing features. It unblocks person-based search and provides immediate value.

**Success Criteria**:
- ‚úÖ Can assign detected faces to person entities
- ‚úÖ Can rename, merge, and split persons via API  
- ‚úÖ Can search "find photos of person X"
- ‚úÖ End-to-end test passes with real photo collection

**Next Milestone After This**: User Experience & Polish
- Web UI for person management
- Hybrid search improvements  
- Smart album creation
- Bulk operations

---

### Cross-Cutting (Updated)
- Observability & Metrics: ‚úÖ /health, /metrics, /metrics.prom (Prometheus exposition), /embedding/backend, average & histogram task durations, queue gauges. ‚ùå Index autosave stats, p95 duration metric export (custom summary), task error code metric.
- Task Queue: ‚úÖ Multi-worker concurrency (thread pool), optimistic locking claim, progress & cancellation (recluster), started_at/finished_at timing, exponential retry w/ jitter backoff & transient/permanent classification, configurable max retries, dead-letter queue (state='dead') + admin requeue (API + CLI). ‚ùå Categorized error codes taxonomy; ‚ùå DLQ metrics (count gauge) pending.
- Vector Index Strategy: ‚úÖ FAISS flat persistent. ‚ùå ANN (HNSW/IVF), delta updates, autosave thread, per-face/person indexes.
- Model Realization: ‚úÖ Image/Text embeddings real. ‚úÖ **Face embeddings REAL** (multiple providers: Facenet, InsightFace, LVFace with subprocess mode). ‚úÖ **Captions REAL** (BLIP2 operational, Qwen2.5-VL has compatibility issues). ‚ùå OCR, event/theme models.
- Data Store: ‚ö†Ô∏è SQLite baseline. ‚ùå Postgres option, backup/export tooling, integrity reconciliation.
- Security / Auth: ‚ùå No auth, no RBAC, no auditing.
- Governance & Compliance: ‚ùå PII purge / face/person deletion workflows.
- Developer Experience: ‚ö†Ô∏è macOS fast smoke path (core-only deps, no migrations/workers by default), split requirements (core vs ML), Dockerfile INCLUDE_ML toggle, compose GPU override for WSL2, Typer CLI (ping/init-db/ingest-scan), VPN troubleshooting note in docs. ‚ùå Coverage for clustering/index, CLI task ops, performance benchmarks, broader tests.
- Scalability: ‚ùå GPU batching, resource limits, sharded re-embed orchestration.
 - API & Schema Consistency: ‚ö†Ô∏è Mixed envelope styles. ‚ùå Unified {api_version,data,meta,error} wrapper, standardized error codes.
- Reliability & Resilience: ‚úÖ Dead-letter queue and requeue endpoint; ‚ùå rate limiting / batching heavy tasks.
 - Real-Time UX Hooks: ‚ùå Websocket/SSE for task progress, live clustering updates.
 - Validation & Safety: ‚ö†Ô∏è Basic path handling. ‚ùå Ingest path/mime/size validation hardening, face/person deletion safety checks.
 - Performance Engineering: ‚ùå Memory-mapped embeddings, batched clustering DB writes.

### Pipeline Completion Gaps (Reference)
**Critical for Working Pipeline:**
1. **Caption pipeline integration** (connect working BLIP2 system to main photo processing)
2. **Caption storage & search** (database schema, text search via captions)
3. **Qwen2.5-VL debugging** (resolve model shape mismatch issues)  
4. **End-to-end testing** with real photos (ingestion ‚Üí detection ‚Üí clustering ‚Üí captions ‚Üí search)

**Secondary Pipeline Features:**
5. Web UI for person management (rename, merge, split workflows)
6. Hybrid search ranking (combine text + image + person + metadata scores)
7. Smart album generation (person-based, location-based, time-based)
8. Enhanced caption models (fix Qwen2.5-VL, add more providers)
9. Metadata enrichment (GPS parsing, better EXIF handling)

**Infrastructure (Post-Pipeline):**
9. ANN index upgrade (HNSW) for large photo collections
10. Postgres migration path for production scale
11. Backup/export tools and restore workflows
12. Auth layer (API tokens, multi-user support)

### Near-Term Focus (Rolling Plan)
Current Track: Option A (Foundation Hardening) ‚Äî initial hardening items completed.

Completed Sprint (Hardening Set 1): ‚úÖ
1. Prometheus exporter (/metrics.prom) with counters/gauges/histogram
2. Multi-worker executor (configurable concurrency) + optimistic claim
3. Retry/backoff (exponential + jitter, transient vs permanent) with Windows timing test stabilized
4. Dead-letter queue + admin & CLI requeue
5. Dev environment split (core vs ml) + dual lock files (requirements-lock-core.txt / requirements-lock-ml.txt) + setup scripts

Completed Sprint (Face Pipeline Set): ‚úÖ
1. **Real face embedding models** ‚Äî Facenet, InsightFace, LVFace providers with configurable selection
2. **LVFace subprocess integration** ‚Äî External model support with isolated environments
3. **Face detection providers** ‚Äî MTCNN + configurable selection (stub/mtcnn/auto)
4. **Production deployment** ‚Äî Docker compose LVFace support, validation, health endpoints
5. **Comprehensive testing** ‚Äî All providers tested, subprocess mode validated

**Current Focus**: See "Next Steps" section above ‚¨ÜÔ∏è

Following Sprint (Preview):
1. Websocket/SSE progress streaming for long tasks
2. Unified API envelope + standardized error taxonomy
3. Input validation hardening (path/MIME/size) & rate limiting heavy GPU ops
4. Index autosave metrics & p95/p99 latency summaries exported
5. GPU batching & embedding batch inference (prework for scalability)

Selection Rationale:
- Improves observability before scaling (reduces blind spots for later performance work).
- Lays groundwork (multi-worker, metrics) required to validate Postgres or ANN performance improvements.

Option Switch Gate:
- After current sprint: evaluate readiness & data volume; if > X assets (configure threshold), prioritize Postgres track earlier.

Success Metrics (for current sprint):
- p95 task duration queryable via Prometheus.
- Can process tasks concurrently without duplicate execution (verified in test).
- Failed task moves to dead-letter after max retries with cause recorded.
- Requeue endpoint returns task to pending and clears retry_count.

Risk / Impact Matrix (Abbrev):
| Item | Impact | Effort | Risk | Notes |
|------|--------|--------|------|-------|
| Prometheus exporter | High | Low | Low | Straightforward instrumentation |
| Multi-worker | High | Med | Med | Need safe locking & idempotency |
| Backoff + DLQ | Med | Med | Low | Schema stable; logic isolated |
| Face embeddings real model | High | Med | Med | Model selection & perf constraints |
| ANN index | High | High | Med | Memory vs recall trade-offs |
| Postgres migration | High | High | High | Data export, migration, integrity checks |

Execution Order justifies quick wins (exporter) before concurrency complexity.

### Legend
‚úÖ Complete   ‚ö†Ô∏è Stub / placeholder   ‚è≥ In progress / partial   ‚ùå Not started

---
_Last updated: 2025-08-16 ‚Äî **MAJOR MILESTONE**: Caption system fully integrated and production-ready. BLIP2 model (13.96 GB) operational with backend subprocess integration, health endpoints validated, server running successfully. Model storage migrated to local directory (20.96 GB total), cache cleaned. Dual environment architecture complete with command-line interface. **Ready for end-to-end testing**. **Prior completed**: Real face embeddings (Facenet, InsightFace, LVFace), LVFace subprocess integration, face detection providers (MTCNN), person management workflow, DLQ & requeue, dual environment, retry/backoff, Prometheus metrics, multi-worker._ 

> Note: Former temporary file `backend/docs/ROADMAP_TEMP.md` has been superseded; its unique items are merged above (dead-letter queue, unified envelope, SSE/websocket progress, validation hardening, rate limiting, memory mapping).
