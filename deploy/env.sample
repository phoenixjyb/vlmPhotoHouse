# Copy this file to `.env` in the deploy/ folder and adjust paths for your host.

# macOS defaults (Docker Desktop -> Settings -> Resources -> File Sharing must include /Users)
PHOTOS_PATH=/Users/$(whoami)/Projects/vlm-photo-engine/data/photos
DERIVED_PATH=/Users/$(whoami)/Projects/vlm-photo-engine/data/derived

# Windows/WSL2 example (uncomment if using WSL2)
# PHOTOS_PATH=/mnt/e/photos
# DERIVED_PATH=/mnt/d/vlm/derived

# SQLite for simplicity (adjust to Postgres if needed)
DATABASE_URL=sqlite:////data/app.sqlite

# Optional worker tuning
WORKER_CONCURRENCY=1
ENABLE_INLINE_WORKER=true

# GPU assignment (NVIDIA indices as seen by Docker). Split workloads:
#   API on GPU 0 (Quadro P2000) for light/interactive tasks; VLM on GPU 1 (RTX 3090) for heavy inference.
API_GPU=0
VLM_GPU=1

# Build toggle: include heavy ML dependencies (torch/faiss/etc.)
# Set to true on WSL2/GPU host; keep false on mac dev for fast builds.
INCLUDE_ML=false

# LVFace Configuration (for real face embeddings)
# If LVFACE_EXTERNAL_DIR is set, external mode is used and LVFACE_MODEL_PATH is ignored.
# Uncomment and adjust paths if you have an external LVFace installation
# FACE_EMBED_PROVIDER=lvface
# LVFACE_PATH=/path/to/your/LVFace
# LVFACE_EXTERNAL_DIR=/lvface
# LVFACE_MODEL_NAME=LVFace-B_Glint360K.onnx
# FACE_EMBED_DIM=512

# Caption Models (BLIP2 / LLaVA / Qwen2.5-VL)
# To run real captioning via an external folder (with its own .venv and inference.py):
# CAPTION_PROVIDER=blip2
# CAPTION_DEVICE=cpu
# CAPTION_MODEL=auto
# CAPTION_EXTERNAL_DIR=/captionmodel-blip

# Video processing (disabled by default)
# VIDEO_ENABLED=false
# VIDEO_EXTENSIONS=.mp4,.mov,.mkv,.avi,.m4v
# VIDEO_KEYFRAME_INTERVAL_SEC=2.0
# VIDEO_SCENE_DETECT=false
# VIDEO_SCENE_MIN_SEC=1.0
